HDFS 

Filesystem that manage the storage across the network is called the DISTRIBUTED FILE SYSTEM , if it falls under some Hadoop Rules then it is called as 
HDFS Hasddop Distributed File Sytem 

HDFS is a filesystem designed for Storing large files with streaming data access patterns running on commodity hardware 

		Very large File => it can store hundred MB TB of data 
		Streaming data access => build on the idea of Write once read many times 
		commodity hardware =>  it is built upon the low/cheap hardware 
		
		
HDFS is not Suitable for the Below Applications 

	low latency => Application need low latency access in the ten of milliseconds ( HDFS is optimised for delivering  high throughput of data and  at the expense of latency )
	lots of small file => since Name node Hold the Filesystem metadata in memory , if we have huge number of small files , metadata  required to store the  images  will be huge resulting reduce performance 
	multiple writes => As of now there is no support for Multiple Writes , we can append the data to the File 
	
HDFS Concepts 

 Filesystem blocks are typically a few kilobytes in size. HDFS has 128MB Block size , 
 Also  a 1 MB file stored with a block size of 128 MB uses 1 MB of disk space, not 128 MB 
 HDFS blocks are large compared to disk blocks, and the reason is to minimize the cost of seeks
 
 
 An HDFS cluster has two types of nodes operating in a master−worker pattern: 		a namenode (the master) and a  		number of datanodes (workers). 
 

		Name Node   - It maintains the Filesystem tree and metadata for all the directories  in the tree  
					  the information is stored in two ways namespace image and  edit logs 
					 name node knows which datanode store which block of information 
		
		Data Node   -  Datanode are workhorse of the namenode They store and retrieve blocks when they are told to (namenode) they report back to the namenode periodically with the list of blocks that they are storing 
		
		Without the namenode the filesystem cant be accessed infact the data would be lost , all the files on the filesystem would be lost since there would be no way of knowing how to reconstruct the files from the blocks on the datanodes.
		It is important to make the name node resilient to failures 
				hdfs federation 
				high availability 
				failover and fencing 
		
			
BLOCK CACHING 
--------------
	Normally a datanode reads blocks from disk, but for frequently accessed files the blocks may be explicitly cached in the datanode’s memory, in an off-heap block cache. 
	
	
HDFS ARCHITECTURE
-------------------

		
		
HDFS READ AND WRITE OPERATION 
------------------------------
By default replication factor is configured by dfs.replication=3 

Selection of DataNode 	
	if the client is part of the cluster , then that will be selected , if the client is not part of the cluster any random node will be selected , 
	second node will be selected on the Different Rack  ,
	Third Node will be selected on the Same Rack of second data node 

NODE DISTANCE :-
------------------
Block is on same rack and same node then the distance is  0 
Block is on same rack and on different node then distance is 2 
Block is on different rack then the distance is 4 
Block is on the Different data center then the distance is 6 



WRITE PROCESS
--------------
	client made a write request to the Namenode by calling the "Create" Method which will make a RPC call to the Namenode to create the file in the namespace with no block associated with it 
	
	Name node performs various Checks  if the file exists or not if the client has valid permisiion or not .otherwise, file creation fails and the client is thrown an
	IOException. if every thing is fine , it will make the record of the new File 
	
	Client Writes the data to the DFSOUTPUTSTREAM which splits the data in to packets and maintain in a internal queue called the data queue 
	
	data queue is consumed by the Data Streamer , Data Streamer asks for the block details to write the Information 
	
	Namenode  will sent the list of datanodes client can write its data <D3,D7,D12> 	Client Connects to the First Data Node and ask the first datanode to form the Pipeline  with the subsequent data nodes . 
	
	client/Data Streamer  writes the data in packets  to first node and first data node forward the date to the second and Third , 
	
	The DFSOutputStream also maintains an internal queue of packets that are waiting to be acknowledged by datanodes, called the ack queue. A packet is removed from the ack queue only when it has been acknowledged by all the datanodes in the pipeline

	Once the data is written data nodesend the Ack back to the Previous while the first data node sends its own ACK to the client 
		
	When the client has finished writing data, it calls close() on the stream (step 6). This action flushes all the remaining packets to the datanode pipeline and waits for 
	acknowledgments before contacting the namenode to signal that the file is complete
	 
	
If any datanode fails while data is being written to it, then the following actions are taken
			First, the pipeline is closed, and any packets in the ack queue are added to the front of the data queue 
			The current block on the good datanodes is given a new identity, which is communicated to the namenode, so that the partial block on the failed datanode will be deleted if the failed datanode recovers later on. 
			The failed datanode is removed from the pipeline, and a new pipeline is constructed from the two good datanodes. The remainder of the block’s data is written to the good datanodes in the pipeline. 
			The namenode notices that the block is under-replicated, and it arranges for a further replica to be created on another node. Subsequent blocks are then treated as normal
		
	



	
READ Process
--------------

	Client Sent the Read request to the Name Node by calling the open method  
	Name node sent the Block details for the data Block1 <D7,D8,D12> Block 2 <D5,D6,D17>
	namenode will send the closet datanode to the Client to the farthest in the order  
	client/DFSInputStream,   would connect to the closet one and start reading the blocks 
	When the end of the block is reached, DFSInputStream will close the connection to the datanode, then find the best datanode for the next block
	
	
	In case of Failures 
		During reading, if the DFSInputStream encounters an error while communicating with a datanode, it will try the next closest one for that block. It will also remember datanodes that have failed so that it doesn’t needlessly retry them for later blocks
		If a corrupted block is found, the DFSInputStream attempts to read a replica of the block from another datanode; it also reports the corrupted block to the namenode.
	

	
	
	
------------------------------------------------------------------------------

FSIMAGE AND EDIT LOGS 
---------------------

To view the FSIMAGE 

sudo hdfs oiv -i /dfs/nn/current/fsimage_0000011 -o fsimage_output.txt

fsimage is a binary file , by using this command text file will be created it has all the snapshot of the filesystem 

 
sudo hdfs oev -i /dfs/nncurrent/edit_000000000 -o edit.xml 




SCALA INTERFACE TO HDFS 
-----------------------



import org.apache.hadoop.fs._
import org.apache.hadoop.conf._


object hdfsFileService{

val conf = new Configuration()
val hdfsCoresitPath= new Path("coresitexml")
val hdfsHdfsSitePath= new Path("hdfs site .xml")
conf.set("fs.defaultFS", "hdfs://quickstart.cloudera:8020")
conf.addResource(hdfsCoresitPath)
conf.addResource(hdfsHdfsSitePath)

val filesytsem = new FileSystem.get(conf)                     // Now New Filesystem has been created with the Proper configuration Files 


// To delete the File 
def removeFile(filename:String) = {
	val path = new path(filename)
	filesystem.remove(path,true)
}


// To create a Folder 
def createFolder(filename:String) ={
	val path = new Path(filename)
	if(!filesystem.exists(path)){
		filesystem.mkdirs(path)
	 }
}

//open the File 

def open(fileName:String) = {
	val path = new Path(fileName)
	filesystem.open(path)
}

//write File 




}
